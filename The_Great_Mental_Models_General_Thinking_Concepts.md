You only think you know, as a matter of fact. And most of your actions are based on incomplete knowledge and you really don’t know what it is all about, or what the purpose of the world is, or know a great deal of other things. It is possible to live and not know.


When understanding is separated from reality, we lose our powers. Understanding must constantly be tested against reality and updated accordingly. This isn’t a box we can tick, a task with a definite beginning and end, but a continuous process.


There are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says “Morning, boys. How’s the water?” And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes “What the hell is water?”


Our failures to update from interacting with reality spring primarily from three things: not having the right perspective or vantage point, ego-induced denial, and distance from the consequences of our decisions.


The further we are from the results of our decisions, the easier it is to keep our current views rather than update them. When you put your hand on a hot stove, you quickly learn the natural consequence. You pay the price for your mistakes. Since you are a pain-avoiding creature, you update your view. Before you touch another stove, you check to see if it’s hot. But you don’t just learn a micro lesson that applies in one situation.


Confucius said, “A man who has committed a mistake and doesn’t correct it, is committing another mistake.”


When it comes to seeing what is—that is, understanding reality—we can follow Charles Darwin’s advice to notice things “which easily escape attention,” and ask why things happened.


It may seem counterintuitive, to work on developing knowledge that is available to everyone, but the universe works in the same way no matter where you are in it. What you need is to understand the principles, so that when the details change you are still able to identify what is really going on.


We are afraid to learn and admit when we don’t know enough. This is the mindset that leads to poor decisions.


When things happen in accord with our view of the world we naturally think they are good for us and others. When they conflict with our views, they are wrong and bad. But the world is smarter than we are and it will teach us all we need to know if we’re open to its feedback—if we keep our feet on the ground.


“To the man with only a hammer, everything starts looking like a nail.”


the description of the thing is not the thing itself. The model is not reality. The abstraction is not the abstracted.


«Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.»


As more and more people know what model you’re using to manipulate them, they may decide not to respond to your incentives. As your competitors gain knowledge of the model, they respond in kind by adopting the model themselves, thus flattening the field.


When ego and not competence drives what we undertake, we have blind spots. If you know what you understand, you know where you have an edge over others. When you are honest about where your knowledge is lacking you know where you are vulnerable and where you can improve. Understanding your circle of competence improves decision-making and outcomes.


«We shall be unable to turn natural advantage to account unless we make use of local guides.»
Sun Tzu2


“A little learning is a dangerous thing;
Drink deep, or taste not the Pierian spring:
There shallow draughts intoxicate the brain,
And drinking largely sobers us again.”6


«Learn from the mistakes of others. You can’t live long enough to make them all yourself.»


Elizabeth I led her country at a time when very few women had public positions of power. It is testament to her strength and intelligence that she was able to admit what she didn’t know and take counsel from others.


if you can’t prove something wrong, you can’t really prove it right either.


tend to assume that the worst that has happened is the worst that can happen, and then prepare for that.


tend to assume that the worst that has happened is the worst that can happen, and then prepare for that. We forget that “the worst” smashed a previous understanding of what was the worst. Therefore, we need to prepare more for the extremes allowable by physics


tend to assume that the worst that has happened is the worst that can happen, and then prepare for that. We forget that “the worst” smashed a previous understanding of what was the worst. Therefore, we need to prepare more for the extremes allowable by physics rather than what has happened until now.


I don’t know what’s the matter with people: they don’t learn by understanding; they learn by some other way—by rote or something. Their knowledge is so fragile!
Richard Feynman


Socratic questioning generally follows this process: Clarifying your thinking and explaining the origins of your ideas. (Why do I think this? What exactly do I think?) Challenging assumptions. (How do I know this is true? What if I thought the opposite?) Looking for evidence. (How can I back this up? What are the sources?) Considering alternative perspectives. (What might others think? How do I know I am correct?) Examining consequences and implications. (What if I am wrong? What are the consequences if I am?) Questioning the original questions. (Why did I think that? Was I correct? What conclusions can I draw from the reasoning process?)


«As to methods, there may be a million and then some, but principles are few. The man who grasps principles can successfully select his own methods.


«As to methods, there may be a million and then some, but principles are few. The man who grasps principles can successfully select his own methods. The man who tries methods, ignoring principles, is sure to have trouble.»


thought experiments are tremendously useful. Imagining physical impossibilities Re-imagining history Intuiting the non-intuitive


Thought experiments tell you about the limits of what you know and the limits of what you should attempt. In order to improve our decision-making and increase our chances of success, we must be willing to probe all of the possibilities we can think of. Thought experiments are not daydreams. They require both rigor and work. But the more you use them, the more you understand actual cause and effect, and the more knowledge you have of what can really be accomplished.


In 1963, the UC Santa Barbara ecologist and economist Garrett Hardin proposed his First Law of Ecology: “You can never merely do one thing.”3 We operate in a world of multiple, overlapping connections, like a web, with many significant, yet obscure and unpredictable, relationships. He developed second-order thinking into a tool, showing that if you don’t consider “the effects of the effects,” you can’t really claim to be doing any thinking at all.


«When we try to pick out anything by itself, we find it hitched to everything else in the Universe.»
John Muir4


Second-order thinking involves asking ourselves if what we are doing now is going to get us the results we want.


The core of Bayesian thinking (or Bayesian updating, as it can be called) is this: given that we have limited but useful information about the world, and are constantly encountering new information, we should probably take into account what we already know when we learn something new. As much of it as possible. Bayesian thinking allows us to use all relevant prior information in making decisions. Statisticians might call it a base rate, taking in outside information about past situations like the one you’re in.


conclusion. Here, a Bayesian analysis indicates you should be concerned. In 1958, 0.93% of the population was diagnosed with diabetes. In 2015 it was 7.4%. When you look at the intervening years, the climb in diabetes diagnosis is steady, not a spike. So the prior relevant data, or priors, indicate a trend that is worrisome. It


conclusion. Here, a Bayesian analysis indicates you should be concerned. In 1958, 0.93% of the population was diagnosed with diabetes. In 2015 it was 7.4%. When you look at the intervening years, the climb in diabetes diagnosis is steady, not a spike. So the prior relevant data, or priors, indicate a trend that is worrisome. It is important to remember that priors themselves are probability estimates. For each bit of prior knowledge, you are not putting it in a binary structure, saying it is true or not. You’re assigning it a probability of being true. Therefore, you can’t let your priors get in the way of processing new knowledge. In Bayesian terms, this is called the likelihood ratio or the Bayes factor. Any new information you encounter


conclusion. Here, a Bayesian analysis indicates you should be concerned. In 1958, 0.93% of the population was diagnosed with diabetes. In 2015 it was 7.4%. When you look at the intervening years, the climb in diabetes diagnosis is steady, not a spike. So the prior relevant data, or priors, indicate a trend that is worrisome. It is important to remember that priors themselves are probability estimates. For each bit of prior knowledge, you are not putting it in a binary structure, saying it is true or not. You’re assigning it a probability of being true. Therefore, you can’t let your priors get in the way of processing new knowledge. In Bayesian terms, this is called the likelihood ratio or the Bayes factor. Any new information you encounter that challenges a prior simply means that the probability of that prior being true may be reduced. Eventually some priors are replaced completely. This is an ongoing cycle of challenging and validating what you believe you know. When making uncertain decisions, it’s nearly always a mistake not to ask: What are the relevant priors? What might I already know that I can use to better understand the reality of the situation?


Conditional probability is similar to Bayesian thinking in practice, but comes at it from a different angle. When you use historical events to predict the future, you have to be mindful of the conditions that surrounded that event. Events can be independent, like tossing a coin, or dependent. In


Fat-tailed curves are different. Take a look. At first glance they seem similar enough. Common outcomes cluster together, creating a wave. The difference is in the tails. In a bell curve the extremes are predictable. There can only be so much deviation from the mean. In a fat-tailed curve there is no real cap on extreme events.


The more extreme events that are possible, the longer the tails of the curve get. Any one extreme event is still unlikely, but the sheer number of options means that we can’t rely on the most common outcomes as representing the average. The more extreme events that are possible, the higher the probability that one of them will occur. Crazy things are definitely going to happen, and we have no way of identifying when.


In a bell curve type of situation, like displaying the distribution of height or weight in a human population, there are outliers on the spectrum of possibility, but the outliers have a fairly well-defined scope. You’ll never meet a man who is ten times the size of an average man.


In a bell curve type of situation, like displaying the distribution of height or weight in a human population, there are outliers on the spectrum of possibility, but the outliers have a fairly well-defined scope. You’ll never meet a man who is ten times the size of an average man. But in a curve with fat tails, like wealth, the central tendency does not work the same way. You may regularly meet people who are ten, 100, or 10,000 times wealthier than the average person.


Asymmetries: Finally, you need to think about something we might call “metaprobability”—the probability that your probability estimates themselves are any good.


In The Black Swan, he argues that any small error in measuring the risk of an extreme event can mean we’re not just slightly off, but way off—off by orders of magnitude, in


We can think about three categories of objects: Ones that are harmed by volatility and unpredictability, ones that are neutral to volatility and unpredictability, and finally, ones that benefit from it. The latter category is antifragile—like a package that wants to be mishandled.


There are two ways to handle such a world: try to predict, or try to prepare. Prediction is tempting. For all of human history, seers and soothsayers have turned a comfortable trade. The problem is that nearly all studies of “expert” predictions in such complex real-world realms as the stock market, geopolitics, and global finance have proven again and again that, for the rare and impactful events in our world, predicting is impossible! It’s more efficient to prepare.


how to fail properly. Failing properly has two major components. First, never take a risk that will do you in completely. (Never get taken out of the game completely.) Second, develop the personal resilience to learn from your failures and start again. With these two rules, you can only fail temporarily.


No one likes to fail. It hurts. But failure carries with it one huge antifragile gift: learning. Those who are not afraid to fail (properly) have a huge advantage over the rest. What they learn makes them less vulnerable to the volatility of the world. They benefit from it, in true antifragile fashion.


one causes the other (causation).


The test of a first-rate intelligence is the ability to hold two opposing ideas in mind at the same time and still retain the ability to function. One should, for example, be able to


The test of a first-rate intelligence is the ability to hold two opposing ideas in mind at the same time and still retain the ability to function. One should, for example, be able to see that things are hopeless yet be determined to make them otherwise.
F. Scott Fitzgerald


see that things are hopeless yet be determined to make them otherwise.
F. Scott Fitzgerald


Avoiding stupidity is easier than seeking brilliance. Combining the ability to think forward and backward allows you to see reality from multiple angles.


Often we focus on positive goals, such as “I want to be rich,” and use this to guide our approach. We make investing and career choices based on our desire to accumulate wealth. We chase after magical solutions, like attempting to outsmart the stock market. These inevitably get us nowhere, and we have usually taken some terrible risks in the process which actually leave us worse off.


Instead, we can try inverting the goal. It becomes, not getting rich, but avoiding being poor. Instead of trying to divine the decisions that will bring wealth, we first try to eliminate those behaviors that are guaranteed to erode it.


«He wins his battles by making no mistakes.»


«He wins his battles by making no mistakes.» 
Sun Tzu


Florence Nightingale to help significantly reduce the mortality rate of British soldiers in military hospitals in the late 19th century. She is often remembered as the founder of modern nursing, but she was also an excellent statistician and was the first woman elected to the Royal Statistical Society in 1858.


«Hence to fight and conquer in all your battles is not supreme excellence; supreme excellence consists in breaking the enemy’s resistance without fighting.» 
Sun Tzu15


Simply invert, always invert, when you are stuck. If you take the results of your inversion seriously, you might make a great deal of progress on solving your problems.


Simpler explanations are more likely to be true than complicated ones. This is the essence of Occam’s Razor, a classic principle of logic and problem-solving.


1933, by Swiss astrophysicist Fritz Zwicky, who coined the phrase “dark matter” to describe a mass we couldn’t see, but which was influencing the behavior of the orbits in the galaxies. Dark matter became the simplest explanation for the observed phenomenon, and Vera Rubin has been credited with providing the first evidence of its existence.


1933, by Swiss astrophysicist Fritz Zwicky, who coined the phrase “dark matter” to describe a mass we couldn’t see, but which was influencing the behavior of the orbits in the galaxies. Dark matter became the simplest explanation for the observed phenomenon, and Vera Rubin has been credited with providing the first evidence of its existence. What is particularly interesting is that to this day no one has ever actually discovered dark matter.


Dark matter is currently the simplest explanation for certain phenomena we observe in the Universe.


Dark matter is currently the simplest explanation for certain phenomena we observe in the Universe. The great thing about science, however, is that it continually seeks to validate its assumptions.


1989 Bengal tigers killed about 60 villagers from India’s Ganges delta.13 No weapons seemed to work against them, including lacing dummies with live wires to shock the tigers away from human populations. Then a student at the Science Club of Calcutta noticed that tigers only attacked when they thought they were unseen, and recalled that the patterns decorating some species of butterflies, beetles, and caterpillars


1989 Bengal tigers killed about 60 villagers from India’s Ganges delta.13 No weapons seemed to work against them, including lacing dummies with live wires to shock the tigers away from human populations. Then a student at the Science Club of Calcutta noticed that tigers only attacked when they thought they were unseen, and recalled that the patterns decorating some species of butterflies, beetles, and caterpillars look like big eyes, ostensibly to trick predators into thinking their prey was also watching them. The result: a human face mask, worn on the back of head. Remarkably, no one wearing a mask was attacked by a tiger for the next three years; anyone killed by tigers during that time had either refused to wear the mask, or had taken it off while working.


I need to listen well so that I hear what is not said.
Thuli Madonsela1


Hard to trace in its origin, Hanlon’s Razor states that we should not attribute to malice that which is more easily explained by stupidity.


We over-conclude based on the available information. We have no trouble packaging in unrelated factors if they happen to occur in proximity to what we already believe.


By not assuming the worst, Vasili Arkhipov single-handedly avoided nuclear war with the Americans.


