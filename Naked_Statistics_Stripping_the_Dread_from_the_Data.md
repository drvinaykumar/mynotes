The Gini index measures how evenly wealth (or income) is shared within a country on a scale from zero to one. The statistic can be calculated for wealth or for annual income, and it can be calculated at the individual level or at the household level. (All of these statistics will be highly correlated but not identical.) The Gini index, like the passer rating, has no intrinsic meaning; it’s a tool for comparison. A country in which every household had identical wealth would have a Gini index of zero. By contrast, a country in which a single household held the country’s entire wealth would have a Gini index of one.


relationship is causal, meaning that a change in one variable is really causing a change in the other.


The first descriptive task is often to find some measure of the “middle” of a set of data, or what statisticians


The first descriptive task is often to find some measure of the “middle” of a set of data, or what statisticians might describe as its “central tendency.”


The most basic measure of the “middle” of a distribution is the mean, or average.


The mean, or average, turns out to have some problems in that regard, namely, that it is prone to distortion by “outliers,” which are observations


The mean, or average, turns out to have some problems in that regard, namely, that it is prone to distortion by “outliers,” which are observations that lie farther from the center.


For distributions without serious outliers, the median and the mean will be similar.


the median divides a distribution in half. The distribution can be further divided into quarters, or quartiles. The first quartile consists of the bottom 25 percent of the observations; the second quartile consists of the next 25 percent of the observations; and so on. Or the distribution can be divided into deciles, each with 10 percent of the observations. (If your income is in the top decile of the American income distribution, you would be earning more than 90 percent of your fellow workers.) We can go even further and divide the distribution into hundredths, or percentiles. Each percentile represents 1 percent of the distribution, so that the 1st percentile represents the bottom 1 percent of the distribution and the 99th percentile represents the top 1 percent of the distribution.


standard deviation, which is a measure of how dispersed the data are from their mean.


The standard deviation is the descriptive statistic that allows us to assign a single number to this dispersion around the mean.


high proportion


far fewer observations lie two standard deviations from the mean, and fewer still lie three or four standard deviations away.


Assume that a department store is selling a dress for $100. The assistant manager marks down all merchandise by 25 percent. But then that assistant manager is fired for hanging out in a bar with Bill Gates,* and the new assistant manager raises all prices by 25 percent. What is the final price of the dress? If you said (or thought) $100, then you had better not skip any paragraphs. The final price of the dress is actually $93.75.


A wage is what we are paid for some fixed amount of labor, such as an hourly or weekly wage. Income is the sum of all payments from different sources.


Mark Twain famously remarked that there are three kinds of lies: lies, damned lies, and statistics.


If an answer is accurate, then more precision is usually better. But no amount of precision can make up for inaccuracy.


As the Economist points out, “If you consider people, not countries, global inequality is falling rapidly.”


Nominal figures are not adjusted for inflation.


Real figures, on the other hand, are adjusted for inflation.


Percentages don’t lie—but they can exaggerate. One way to make growth look explosive is to use percentage change to describe some change relative to a very low starting point.


A correlation of 1, often described as perfect correlation, means that every change in one variable is associated with an equivalent change in the other variable in the same direction.


The formula for calculating the correlation coefficient does the following: 1. Calculates the mean and standard deviation for both variables.


Converts all the data so that each observation is represented by its distance (in standard deviations) from the mean.


The formula then calculates the relationship between height and weight across all the individuals in the sample as measured by standard units.


correlation does not imply causation; a positive or negative association between two variables does not necessarily mean that a change in one of the variables is causing the change in the other.


APPENDIX TO CHAPTER 4 To calculate the correlation coefficient between two sets of numbers, you would perform the following steps, each of which is illustrated by use of the data on heights and weights for 15 hypothetical students in the table below. 1. Convert the height of each student to standard units: (height – mean)/standard deviation. 2. Convert the weight of each student to standard units: (weight – mean)/standard deviation. 3. Calculate the product for each student of (weight in standard units) × (height in standard units). You should see that this number will be largest in absolute value when a student’s height and weight are both relatively far from the mean. 4. The correlation coefficient is the sum of the products calculated above divided by the number of observations (15 in this case).


(More than 99 percent of all DNA is identical among all humans.)


probability of Event A happening and Event B happening is the probability of Event A multiplied by the probability of Event B.


Suppose you are interested in the probability that one event happens or another event happens: outcome A or outcome B (again assuming that they are independent). In this case, the probability of getting A or B consists of the sum of their individual probabilities: the probability of A plus the probability of B.


(Inside trading involves illegally using private information, such as a law firm’s knowledge of an impending corporate acquisition, to trade stock or other securities in the affected companies.)


Statistics cannot be any smarter than the people who use them. And in some cases, they can make smart people do dumb things.


Very little attention was devoted to the “tail risk,” the small risk (named for the tail of the distribution) of some catastrophic outcome.


The VaR models were just like my golf range finder when it was set to meters instead of yards: exact and wrong. The


if you flip a fair coin 1,000,000 times and get 1,000,000 heads in a row, the probability of getting tails on the next flip is still ½.


Here is an exercise that I do with my students to make the same basic point. The larger the class, the better it works. I ask everyone in the class to take out a coin and stand up. We all flip the coin; anyone who flips heads must sit down. Assuming we start with 100 students, roughly 50 will sit down after the first flip. Then we do it again, after which 25 or so are still standing. And so on. More often than not, there will be a student standing at the end who has flipped five or six tails in a row. At that point, I ask the student questions like “How did you do it?” and “What are the best training exercises for flipping so many tails in a row?” or “Is there a special diet that helped you pull off this impressive accomplishment?”

Notes: 1) An anomalous event occured with theperson standing till last. 


reversion to the mean. Probability tells us that any outlier—an observation that is particularly far from the mean in one direction or the other—is likely to be followed by outcomes that are more consistent with the long-term average.


One way to think about this mean reversion is that performance—both mental and physical—consists of some underlying talent-related effort plus an element of luck, good or bad. (Statisticians would call this random error.)


Malmendier and Tate write, “Our results suggest that media-induced superstar culture leads to behavioral distortions beyond mere mean reversion.”


The broader point here is that our ability to analyze data has grown far more sophisticated than our thinking about what we ought to do with the results.


even the finest recipe isn’t going to salvage a meal that begins with spoiled ingredients. So it is with statistics; no amount of fancy analysis can make up for fundamentally flawed data. Hence the expression “garbage in, garbage out.”


you can envision sampling a pot of soup with a single spoonful. If you’ve stirred your soup adequately, a single spoonful can


you can envision sampling a pot of soup with a single spoonful. If you’ve stirred your soup adequately, a single spoonful can tell you how the whole pot tastes.


One crucial caveat is that a bigger sample will not make up for errors in its composition, or “bias.” A bad sample is a bad sample.


cyanobacteria. (These bacteria are blue, and they are the only kind of bacteria that get their energy from photosynthesis; hence the original description of the disease as “blue-green algae.”)


As polls with good samples get larger, they get better, since the margin of error shrinks. As polls with bad samples get larger, the pile of garbage just gets bigger and smellier.


Recall bias is one reason that longitudinal studies are often preferred to cross-sectional studies. In a longitudinal study the data are collected contemporaneously. At age five, a participant can be asked about his attitudes toward school. Then, thirteen years later, we can revisit that same participant and determine whether he has dropped out of high school. In a cross-sectional study, in which all the data are collected at one point in time, we must ask an eighteen-year-old high school dropout how he or she felt about school at age five, which is inherently less reliable.


(If you have a room of people with varying heights, forcing the short people to leave will raise the average height in the room, but it doesn’t make anyone taller.)


The core principle underlying the central limit theorem is that a large, properly drawn sample will resemble the population from which it is drawn.


The central limit theorem tells us that the sample means will be distributed roughly as a normal distribution around the population mean. The


(As a rule of thumb, the sample size must be at least 30 for the central limit theorem to hold true.)


The standard error measures the dispersion of the sample means. How tightly do we expect the sample


The standard error measures the dispersion of the sample means. How tightly do we expect the sample means to cluster around the population mean? There is some potential confusion here, as we have now introduced two different measures of dispersion: the standard deviation and the standard error. Here is what you need to remember to keep them straight: 1. The standard deviation measures dispersion in the underlying population. In this case, it might measure the dispersion of the weights of all the participants in the Framingham Heart Study, or the dispersion around the mean for the entire marathon field. 2. The standard error measures the dispersion of the sample means. If we draw repeated samples of 100 participants from the Framingham Heart Study, what will the dispersion of those sample means look like? 3. Here is what ties the two concepts together: The standard error is the standard deviation of the sample means!

Notes: 1) Standard error 


The standard error measures the dispersion of the sample means. How tightly do we expect the sample means to cluster around the population mean? There is some potential confusion here, as we have now introduced two different measures of dispersion: the standard deviation and the standard error. Here is what you need to remember to keep them straight: 1. The standard deviation measures dispersion in the underlying population. In this case, it might measure the dispersion of the weights of all the participants in the Framingham Heart Study, or the dispersion around the mean for the entire marathon field. 2. The standard error measures the dispersion of the sample means. If we draw repeated samples of 100 participants from the Framingham Heart Study, what will the dispersion of those sample means look like? 3. Here is what ties the two concepts together: The standard error is the standard deviation of the sample means! Isn’t that kind of cool?


SE where s is the standard deviation of the population from which the sample is drawn, and n is the size of the sample.


Because the sample means are distributed normally (thanks to the central limit theorem), we can harness the power of the normal curve. We expect that roughly 68 percent of all sample means will lie within one standard error of the population mean; 95 percent of the sample means will lie within two standard errors of the population mean; and 99.7 percent of the sample means will lie within three standard errors of the population mean.


To be more precise, any statistical inference begins with an implicit or explicit null hypothesis. This is our starting assumption, which will be rejected or not on the basis of subsequent statistical analysis. If we reject the null hypothesis, then we typically accept some alternative hypothesis that is more consistent with the data observed. For example, in a court of law the starting assumption, or null hypothesis, is that the defendant is innocent. The job of the prosecution is to persuade the judge or jury to reject that assumption and


It may seem counterintuitive, but researchers often create a null hypothesis in hopes of being able to reject it.


One of the most common thresholds that researchers use for rejecting a null hypothesis is 5 percent, which is often written in decimal form: .05. This probability is known as a significance level, and it represents the upper bound for the likelihood of observing some pattern of data if the null hypothesis were true.


